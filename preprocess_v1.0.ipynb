{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POJnKO9bzlgR"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CA6000 (Kaggle PS S5E12) — Data Cleaning & Preprocessing\n",
    "# Output: X_train_proc, X_val_proc, y_train, y_val, X_test_proc\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "import scipy.sparse as sp\n",
    "\n",
    "SEED = 42\n",
    "TARGET_COL = \"diagnosed_diabetes\"\n",
    "ID_COL = \"id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUlDK2o4z9UO",
    "outputId": "b58ee7bc-d9c9-4c1d-e34a-6b74046902d9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 1) Robust path resolver (Kaggle / Colab / local / /mnt/data)\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_dataset_paths(prefer_dir=\"/content\"):\n",
    "    candidates = [Path(prefer_dir), Path(\"/mnt/data\"), Path(\".\")]\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if kaggle_input.exists():\n",
    "        candidates.append(kaggle_input)\n",
    "\n",
    "    def find_file(root: Path, filename: str):\n",
    "        direct = root / filename\n",
    "        if direct.exists():\n",
    "            return direct\n",
    "        hits = list(root.rglob(filename))\n",
    "        return hits[0] if hits else None\n",
    "\n",
    "    train_path = test_path = sub_path = None\n",
    "    for root in candidates:\n",
    "        tp = find_file(root, \"train.csv\")\n",
    "        te = find_file(root, \"test.csv\")\n",
    "        ss = find_file(root, \"sample_submission.csv\")\n",
    "        if tp is not None and te is not None:\n",
    "            train_path, test_path, sub_path = tp, te, ss\n",
    "            break\n",
    "\n",
    "    if train_path is None or test_path is None:\n",
    "        raise FileNotFoundError(\"Cannot find train.csv/test.csv under preferred dirs.\")\n",
    "\n",
    "    return str(train_path), str(test_path), (str(sub_path) if sub_path else None)\n",
    "\n",
    "TRAIN_PATH, TEST_PATH, SUB_PATH = resolve_dataset_paths(\"/content\")\n",
    "print(TRAIN_PATH, TEST_PATH, SUB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TC-yOfGb0PZ1",
    "outputId": "b7c24fa2-d17f-4c69-f599-c97911ad6024"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Load data\n",
    "# ----------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"train:\", train_df.shape)\n",
    "print(\"test :\", test_df.shape)\n",
    "print(\"\\nTrain head:\")\n",
    "display(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9ENgPAy0UZ4",
    "outputId": "b58fdd38-d073-4b0f-baac-c2546f6dfad8"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Data audit & sanity checks (good for your report)\n",
    "# ----------------------------\n",
    "def basic_audit(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    # Required columns\n",
    "    assert TARGET_COL in train_df.columns, f\"Missing target '{TARGET_COL}' in train.csv\"\n",
    "    assert ID_COL in train_df.columns and ID_COL in test_df.columns, \"Missing 'id' in train/test\"\n",
    "\n",
    "    # Column alignment (except target)\n",
    "    train_features = [c for c in train_df.columns if c != TARGET_COL]\n",
    "    assert set(train_features) == set(test_df.columns), \"Train features != Test columns (schema mismatch)\"\n",
    "\n",
    "    # ID uniqueness\n",
    "    assert train_df[ID_COL].is_unique, \"Train id is not unique\"\n",
    "    assert test_df[ID_COL].is_unique, \"Test id is not unique\"\n",
    "\n",
    "    # Duplicates\n",
    "    dup_train = train_df.duplicated().sum()\n",
    "    dup_test = test_df.duplicated().sum()\n",
    "\n",
    "    # Missing summary\n",
    "    miss_train = (train_df.isnull().mean().sort_values(ascending=False))\n",
    "    miss_test = (test_df.isnull().mean().sort_values(ascending=False))\n",
    "\n",
    "    # Target check\n",
    "    y = train_df[TARGET_COL]\n",
    "    # Ensure binary-like\n",
    "    unique_y = sorted(y.dropna().unique().tolist())\n",
    "\n",
    "    print(\"\\n[Audit] duplicates:\", {\"train\": int(dup_train), \"test\": int(dup_test)})\n",
    "    print(\"[Audit] top missing rate (train):\")\n",
    "    print(miss_train.head(10))\n",
    "    print(\"[Audit] top missing rate (test):\")\n",
    "    print(miss_test.head(10))\n",
    "    print(\"[Audit] target unique values:\", unique_y)\n",
    "    print(\"[Audit] target distribution:\\n\", y.value_counts(dropna=False))\n",
    "\n",
    "basic_audit(train_df, test_df)\n",
    "\n",
    "# Convert target to int (0/1)\n",
    "train_df[TARGET_COL] = train_df[TARGET_COL].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0rjKkMu0aac",
    "outputId": "57cb9781-588d-440e-9545-3211c9a24044"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 4) Define column groups\n",
    "# ----------------------------\n",
    "# Categorical columns (object/string)\n",
    "cat_cols = train_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c != ID_COL]  # ensure id is not treated as category\n",
    "\n",
    "# Binary columns (known 0/1 flags in this dataset)\n",
    "bin_cols = [\"family_history_diabetes\", \"hypertension_history\", \"cardiovascular_history\"]\n",
    "bin_cols = [c for c in bin_cols if c in train_df.columns]\n",
    "\n",
    "# Numeric columns = all numeric excluding id/target/binary\n",
    "num_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in [ID_COL, TARGET_COL] + bin_cols]\n",
    "\n",
    "print(\"\n",
    "Column groups:\")\n",
    "print(\"num_cols:\", num_cols)\n",
    "print(\"bin_cols:\", bin_cols)\n",
    "print(\"cat_cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSrlmfv80kFe"
   },
   "outputs": [],
   "source": [
    "# Optional: verify binary columns truly contain only 0/1\n",
    "for c in bin_cols:\n",
    "    bad_vals = set(train_df[c].dropna().unique()) - {0, 1}\n",
    "    if bad_vals:\n",
    "        raise ValueError(f\"Binary col '{c}' has unexpected values: {bad_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "PvCpOa2-0mwB",
    "outputId": "fc6dd958-3fef-4929-bb36-fcc0d3dd673d"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5) (Optional but nice) Range check for numeric columns\n",
    "# ----------------------------\n",
    "def numeric_range_report(df: pd.DataFrame, columns):\n",
    "    desc = df[columns].describe(percentiles=[0.01, 0.5, 0.99]).T\n",
    "    # Keep a compact view\n",
    "    return desc[[\"min\", \"1%\", \"50%\", \"99%\", \"max\", \"mean\", \"std\"]].sort_values(\"max\", ascending=False)\n",
    "\n",
    "range_report = numeric_range_report(train_df, num_cols)\n",
    "print(\"\\nNumeric range report (top 8 by max):\")\n",
    "display(range_report.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ufJOory0rgt",
    "outputId": "b9fb0571-46d6-4cb7-aab7-948a34e782c5"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6) Split data BEFORE fitting preprocessors (avoid leakage)\n",
    "# ----------------------------\n",
    "X = train_df.drop(columns=[TARGET_COL])\n",
    "y = train_df[TARGET_COL].values.astype(np.int32)\n",
    "\n",
    "train_ids = X[ID_COL].values\n",
    "test_ids = test_df[ID_COL].values\n",
    "\n",
    "X = X.drop(columns=[ID_COL])\n",
    "X_test = test_df.drop(columns=[ID_COL])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nSplit shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
    "print(\"X_test :\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kApZj2Xl0vK2"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7) Custom transformer: quantile clipping for numeric outliers\n",
    "#    (fit on training only)\n",
    "# ----------------------------\n",
    "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_q=0.005, upper_q=0.995):\n",
    "        self.lower_q = lower_q\n",
    "        self.upper_q = upper_q\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        self.lower_ = np.nanquantile(X, self.lower_q, axis=0)\n",
    "        self.upper_ = np.nanquantile(X, self.upper_q, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return np.clip(X, self.lower_, self.upper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2CIX_jZ0w15"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 8) Build preprocessing pipeline\n",
    "#    - numeric: median impute -> clip -> standardize\n",
    "#    - binary : most_frequent impute (keep 0/1)\n",
    "#    - cate   : most_frequent impute -> one-hot (sparse)\n",
    "# ----------------------------\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"clipper\", QuantileClipper(lower_q=0.005, upper_q=0.995)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "binary_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "# Keep output sparse for efficiency; handle sklearn <1.2 fallback\n",
    "try:\n",
    "    categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", categorical_encoder)\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"bin\", binary_pipe, bin_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,  # prefer sparse output so XGBoost can consume efficiently\n",
    "    verbose_feature_names_out=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOPDpRsG00qJ",
    "outputId": "4326cff2-35bd-4b94-d0cf-51f0930fe4bb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 9) Fit on train, transform val/test\n",
    "# ----------------------------\n",
    "X_train_proc = preprocess.fit_transform(X_train)\n",
    "X_val_proc = preprocess.transform(X_val)\n",
    "X_test_proc = preprocess.transform(X_test)\n",
    "\n",
    "# Cast to float32 for models (works for dense or sparse matrices)\n",
    "X_train_proc = X_train_proc.astype(np.float32)\n",
    "X_val_proc = X_val_proc.astype(np.float32)\n",
    "X_test_proc = X_test_proc.astype(np.float32)\n",
    "\n",
    "print(\"\n",
    "Processed shapes:\")\n",
    "print(\"X_train_proc:\", X_train_proc.shape)\n",
    "print(\"X_val_proc  :\", X_val_proc.shape)\n",
    "print(\"X_test_proc :\", X_test_proc.shape)\n",
    "\n",
    "# Safety checks (support sparse/dense)\n",
    "def assert_no_nan(arr, name):\n",
    "    if sp.issparse(arr):\n",
    "        assert not np.isnan(arr.data).any(), f\"NaNs remain in {name}\"\n",
    "    else:\n",
    "        assert not np.isnan(arr).any(), f\"NaNs remain in {name}\"\n",
    "\n",
    "assert_no_nan(X_train_proc, \"X_train_proc\")\n",
    "assert_no_nan(X_val_proc, \"X_val_proc\")\n",
    "assert_no_nan(X_test_proc, \"X_test_proc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLJzVJaO037y",
    "outputId": "d6c948bb-a4e4-492a-8770-d2ed8c0037d0"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 10) Save artifacts for reproducibility\n",
    "# ----------------------------\n",
    "artifact = {\n",
    "    \"id_col\": ID_COL,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"num_cols\": num_cols,\n",
    "    \"bin_cols\": bin_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"preprocess\": preprocess,\n",
    "}\n",
    "\n",
    "joblib.dump(artifact, \"preprocess_artifact.joblib\")\n",
    "print(\"\\nSaved preprocess artifact -> preprocess_artifact.joblib\")\n",
    "\n",
    "# Optional: save processed arrays (may be large, enable if you want)\n",
    "# np.save(\"X_train_proc.npy\", X_train_proc)\n",
    "# np.save(\"X_val_proc.npy\", X_val_proc)\n",
    "# np.save(\"X_test_proc.npy\", X_test_proc)\n",
    "# np.save(\"y_train.npy\", y_train)\n",
    "# np.save(\"y_val.npy\", y_val)\n",
    "\n",
    "print(\"\\n✅ Ready for model training stage:\")\n",
    "print(\"Use X_train_proc, y_train, X_val_proc, y_val, X_test_proc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 11) Train XGBoost (AUC metric, sparse-friendly)\n",
    "# ----------------------------\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scale_pos = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",  # use \"gpu_hist\" if GPU is available\n",
    "    scale_pos_weight=scale_pos,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    eval_set=[(X_val_proc, y_val)],\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=200,\n",
    ")\n",
    "\n",
    "val_pred = model.predict_proba(X_val_proc)[:, 1]\n",
    "print(\"VAL AUC:\", roc_auc_score(y_val, val_pred))\n",
    "\n",
    "# Prepare submission on demand\n",
    "# test_pred = model.predict_proba(X_test_proc)[:, 1]\n",
    "# submission = pd.DataFrame({ID_COL: test_ids, TARGET_COL: test_pred})\n",
    "# submission.to_csv(\"submission_xgb.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
