{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91723,"databundleVersionId":14272474,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# CA6000 (Kaggle PS S5E12) — Data Cleaning & Preprocessing\n# Output: X_train_proc, X_val_proc, y_train, y_val, X_test_proc\n# ============================================================\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nimport joblib\nimport scipy.sparse as sp\n\nSEED = 42\nTARGET_COL = \"diagnosed_diabetes\"\nID_COL = \"id\"\n","metadata":{"id":"POJnKO9bzlgR","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:21.945146Z","iopub.execute_input":"2025-12-26T12:20:21.945467Z","iopub.status.idle":"2025-12-26T12:20:22.867546Z","shell.execute_reply.started":"2025-12-26T12:20:21.945435Z","shell.execute_reply":"2025-12-26T12:20:22.866751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------\n# 1) Robust path resolver (Kaggle / Colab / local / /mnt/data)\n# ----------------------------\nfrom pathlib import Path\n\ndef resolve_dataset_paths(prefer_dir=\"/content\"):\n    candidates = [Path(prefer_dir), Path(\"/mnt/data\"), Path(\".\")]\n\n    kaggle_input = Path(\"/kaggle/input\")\n    if kaggle_input.exists():\n        candidates.append(kaggle_input)\n\n    def find_file(root: Path, filename: str):\n        direct = root / filename\n        if direct.exists():\n            return direct\n        hits = list(root.rglob(filename))\n        return hits[0] if hits else None\n\n    train_path = test_path = sub_path = None\n    for root in candidates:\n        tp = find_file(root, \"train.csv\")\n        te = find_file(root, \"test.csv\")\n        ss = find_file(root, \"sample_submission.csv\")\n        if tp is not None and te is not None:\n            train_path, test_path, sub_path = tp, te, ss\n            break\n\n    if train_path is None or test_path is None:\n        raise FileNotFoundError(\"Cannot find train.csv/test.csv under preferred dirs.\")\n\n    return str(train_path), str(test_path), (str(sub_path) if sub_path else None)\n\nTRAIN_PATH, TEST_PATH, SUB_PATH = resolve_dataset_paths(\"/content\")\nprint(TRAIN_PATH, TEST_PATH, SUB_PATH)","metadata":{"id":"HUlDK2o4z9UO","outputId":"b58ee7bc-d9c9-4c1d-e34a-6b74046902d9","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:22.868829Z","iopub.execute_input":"2025-12-26T12:20:22.869196Z","iopub.status.idle":"2025-12-26T12:20:22.882560Z","shell.execute_reply.started":"2025-12-26T12:20:22.869170Z","shell.execute_reply":"2025-12-26T12:20:22.881910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 2) Load data\n# ----------------------------\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\nprint(\"\\nShapes:\")\nprint(\"train:\", train_df.shape)\nprint(\"test :\", test_df.shape)\nprint(\"\\nTrain head:\")\ndisplay(train_df.head(3))","metadata":{"id":"TC-yOfGb0PZ1","outputId":"b7c24fa2-d17f-4c69-f599-c97911ad6024","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:22.883356Z","iopub.execute_input":"2025-12-26T12:20:22.883626Z","iopub.status.idle":"2025-12-26T12:20:24.686113Z","shell.execute_reply.started":"2025-12-26T12:20:22.883606Z","shell.execute_reply":"2025-12-26T12:20:24.685423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 3) Data audit & sanity checks (good for your report)\n# ----------------------------\ndef basic_audit(train_df: pd.DataFrame, test_df: pd.DataFrame):\n    # Required columns\n    assert TARGET_COL in train_df.columns, f\"Missing target '{TARGET_COL}' in train.csv\"\n    assert ID_COL in train_df.columns and ID_COL in test_df.columns, \"Missing 'id' in train/test\"\n\n    # Column alignment (except target)\n    train_features = [c for c in train_df.columns if c != TARGET_COL]\n    assert set(train_features) == set(test_df.columns), \"Train features != Test columns (schema mismatch)\"\n\n    # ID uniqueness\n    assert train_df[ID_COL].is_unique, \"Train id is not unique\"\n    assert test_df[ID_COL].is_unique, \"Test id is not unique\"\n\n    # Duplicates\n    dup_train = train_df.duplicated().sum()\n    dup_test = test_df.duplicated().sum()\n\n    # Missing summary\n    miss_train = (train_df.isnull().mean().sort_values(ascending=False))\n    miss_test = (test_df.isnull().mean().sort_values(ascending=False))\n\n    # Target check\n    y = train_df[TARGET_COL]\n    # Ensure binary-like\n    unique_y = sorted(y.dropna().unique().tolist())\n\n    print(\"\\n[Audit] duplicates:\", {\"train\": int(dup_train), \"test\": int(dup_test)})\n    print(\"[Audit] top missing rate (train):\")\n    print(miss_train.head(10))\n    print(\"[Audit] top missing rate (test):\")\n    print(miss_test.head(10))\n    print(\"[Audit] target unique values:\", unique_y)\n    print(\"[Audit] target distribution:\\n\", y.value_counts(dropna=False))\n\nbasic_audit(train_df, test_df)\n\n# Convert target to int (0/1)\ntrain_df[TARGET_COL] = train_df[TARGET_COL].astype(int)","metadata":{"id":"m9ENgPAy0UZ4","outputId":"b58fdd38-d073-4b0f-baac-c2546f6dfad8","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:24.687263Z","iopub.execute_input":"2025-12-26T12:20:24.687532Z","iopub.status.idle":"2025-12-26T12:20:25.633599Z","shell.execute_reply.started":"2025-12-26T12:20:24.687511Z","shell.execute_reply":"2025-12-26T12:20:25.632846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------\n# 4) Define column groups\n# ----------------------------\n# Categorical columns (object/string)\ncat_cols = train_df.select_dtypes(include=[\"object\"]).columns.tolist()\ncat_cols = [c for c in cat_cols if c != ID_COL]  # ensure id is not treated as category\n\n# Binary columns (known 0/1 flags in this dataset)\nbin_cols = [\"family_history_diabetes\", \"hypertension_history\", \"cardiovascular_history\"]\nbin_cols = [c for c in bin_cols if c in train_df.columns]\n\n# Numeric columns = all numeric excluding id/target/binary\nnum_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\nnum_cols = [c for c in num_cols if c not in [ID_COL, TARGET_COL] + bin_cols]\n\nprint(\"Column groups:\")\nprint(\"num_cols:\", num_cols)\nprint(\"bin_cols:\", bin_cols)\nprint(\"cat_cols:\", cat_cols)\n","metadata":{"id":"u0rjKkMu0aac","outputId":"57cb9781-588d-440e-9545-3211c9a24044","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:25.635308Z","iopub.execute_input":"2025-12-26T12:20:25.635569Z","iopub.status.idle":"2025-12-26T12:20:25.758367Z","shell.execute_reply.started":"2025-12-26T12:20:25.635540Z","shell.execute_reply":"2025-12-26T12:20:25.757624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: verify binary columns truly contain only 0/1\nfor c in bin_cols:\n    bad_vals = set(train_df[c].dropna().unique()) - {0, 1}\n    if bad_vals:\n        raise ValueError(f\"Binary col '{c}' has unexpected values: {bad_vals}\")","metadata":{"id":"zSrlmfv80kFe","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:25.759510Z","iopub.execute_input":"2025-12-26T12:20:25.759767Z","iopub.status.idle":"2025-12-26T12:20:25.778848Z","shell.execute_reply.started":"2025-12-26T12:20:25.759745Z","shell.execute_reply":"2025-12-26T12:20:25.777992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 5) (Optional but nice) Range check for numeric columns\n# ----------------------------\ndef numeric_range_report(df: pd.DataFrame, columns):\n    desc = df[columns].describe(percentiles=[0.01, 0.5, 0.99]).T\n    # Keep a compact view\n    return desc[[\"min\", \"1%\", \"50%\", \"99%\", \"max\", \"mean\", \"std\"]].sort_values(\"max\", ascending=False)\n\nrange_report = numeric_range_report(train_df, num_cols)\nprint(\"\\nNumeric range report (top 8 by max):\")\ndisplay(range_report.head(8))\n","metadata":{"id":"PvCpOa2-0mwB","outputId":"fc6dd958-3fef-4929-bb36-fcc0d3dd673d","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:25.779860Z","iopub.execute_input":"2025-12-26T12:20:25.780150Z","iopub.status.idle":"2025-12-26T12:20:26.097320Z","shell.execute_reply.started":"2025-12-26T12:20:25.780121Z","shell.execute_reply":"2025-12-26T12:20:26.096603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 6) Split data BEFORE fitting preprocessors (avoid leakage)\n# ----------------------------\nX = train_df.drop(columns=[TARGET_COL])\ny = train_df[TARGET_COL].values.astype(np.int32)\n\ntrain_ids = X[ID_COL].values\ntest_ids = test_df[ID_COL].values\n\nX = X.drop(columns=[ID_COL])\nX_test = test_df.drop(columns=[ID_COL])\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=SEED,\n    stratify=y\n)\n\nprint(\"\\nSplit shapes:\")\nprint(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\nprint(\"X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\nprint(\"X_test :\", X_test.shape)\n","metadata":{"id":"9ufJOory0rgt","outputId":"b9fb0571-46d6-4cb7-aab7-948a34e782c5","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:26.098181Z","iopub.execute_input":"2025-12-26T12:20:26.098426Z","iopub.status.idle":"2025-12-26T12:20:26.697091Z","shell.execute_reply.started":"2025-12-26T12:20:26.098383Z","shell.execute_reply":"2025-12-26T12:20:26.696422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 7) Custom transformer: quantile clipping for numeric outliers\n#    (fit on training only)\n# ----------------------------\nclass QuantileClipper(BaseEstimator, TransformerMixin):\n    def __init__(self, lower_q=0.005, upper_q=0.995):\n        self.lower_q = lower_q\n        self.upper_q = upper_q\n\n    def fit(self, X, y=None):\n        X = np.asarray(X, dtype=float)\n        self.lower_ = np.nanquantile(X, self.lower_q, axis=0)\n        self.upper_ = np.nanquantile(X, self.upper_q, axis=0)\n        return self\n\n    def transform(self, X):\n        X = np.asarray(X, dtype=float)\n        return np.clip(X, self.lower_, self.upper_)","metadata":{"id":"kApZj2Xl0vK2","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:26.697930Z","iopub.execute_input":"2025-12-26T12:20:26.698130Z","iopub.status.idle":"2025-12-26T12:20:26.703595Z","shell.execute_reply.started":"2025-12-26T12:20:26.698111Z","shell.execute_reply":"2025-12-26T12:20:26.702654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------\n# 8) Build preprocessing pipeline\n#    - numeric: median impute -> clip -> standardize\n#    - binary : most_frequent impute (keep 0/1)\n#    - cate   : most_frequent impute -> one-hot (sparse)\n# ----------------------------\nnumeric_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"clipper\", QuantileClipper(lower_q=0.005, upper_q=0.995)),\n    (\"scaler\", StandardScaler())\n])\n\nbinary_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n])\n\n# Keep output sparse for efficiency; handle sklearn <1.2 fallback\ntry:\n    categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\nexcept TypeError:\n    categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n\ncategorical_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", categorical_encoder)\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, num_cols),\n        (\"bin\", binary_pipe, bin_cols),\n        (\"cat\", categorical_pipe, cat_cols),\n    ],\n    remainder=\"drop\",\n    sparse_threshold=1.0,  # prefer sparse output so XGBoost can consume efficiently\n    verbose_feature_names_out=False\n)\n","metadata":{"id":"G2CIX_jZ0w15","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:26.704631Z","iopub.execute_input":"2025-12-26T12:20:26.704855Z","iopub.status.idle":"2025-12-26T12:20:26.717352Z","shell.execute_reply.started":"2025-12-26T12:20:26.704827Z","shell.execute_reply":"2025-12-26T12:20:26.716757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------\n# 9) Fit on train, transform val/test\n# ----------------------------\nX_train_proc = preprocess.fit_transform(X_train)\nX_val_proc = preprocess.transform(X_val)\nX_test_proc = preprocess.transform(X_test)\n\n# Cast to float32 for models (works for dense or sparse matrices)\nX_train_proc = X_train_proc.astype(np.float32)\nX_val_proc = X_val_proc.astype(np.float32)\nX_test_proc = X_test_proc.astype(np.float32)\n\nprint(\"Processed shapes:\")\nprint(\"X_train_proc:\", X_train_proc.shape)\nprint(\"X_val_proc  :\", X_val_proc.shape)\nprint(\"X_test_proc :\", X_test_proc.shape)\n\n# Safety checks (support sparse/dense)\ndef assert_no_nan(arr, name):\n    if sp.issparse(arr):\n        assert not np.isnan(arr.data).any(), f\"NaNs remain in {name}\"\n    else:\n        assert not np.isnan(arr).any(), f\"NaNs remain in {name}\"\n\nassert_no_nan(X_train_proc, \"X_train_proc\")\nassert_no_nan(X_val_proc, \"X_val_proc\")\nassert_no_nan(X_test_proc, \"X_test_proc\")\n","metadata":{"id":"zOPDpRsG00qJ","outputId":"4326cff2-35bd-4b94-d0cf-51f0930fe4bb","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:26.718243Z","iopub.execute_input":"2025-12-26T12:20:26.718552Z","iopub.status.idle":"2025-12-26T12:20:30.431202Z","shell.execute_reply.started":"2025-12-26T12:20:26.718516Z","shell.execute_reply":"2025-12-26T12:20:30.430464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 10) Save artifacts for reproducibility\n# ----------------------------\nartifact = {\n    \"id_col\": ID_COL,\n    \"target_col\": TARGET_COL,\n    \"num_cols\": num_cols,\n    \"bin_cols\": bin_cols,\n    \"cat_cols\": cat_cols,\n    \"preprocess\": preprocess,\n}\n\njoblib.dump(artifact, \"preprocess_artifact.joblib\")\nprint(\"\\nSaved preprocess artifact -> preprocess_artifact.joblib\")\n\n# Optional: save processed arrays (may be large, enable if you want)\n# np.save(\"X_train_proc.npy\", X_train_proc)\n# np.save(\"X_val_proc.npy\", X_val_proc)\n# np.save(\"X_test_proc.npy\", X_test_proc)\n# np.save(\"y_train.npy\", y_train)\n# np.save(\"y_val.npy\", y_val)\n\nprint(\"\\n✅ Ready for model training stage:\")\nprint(\"Use X_train_proc, y_train, X_val_proc, y_val, X_test_proc\")","metadata":{"id":"fLJzVJaO037y","outputId":"d6c948bb-a4e4-492a-8770-d2ed8c0037d0","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:20:30.432104Z","iopub.execute_input":"2025-12-26T12:20:30.432326Z","iopub.status.idle":"2025-12-26T12:20:30.440461Z","shell.execute_reply.started":"2025-12-26T12:20:30.432305Z","shell.execute_reply":"2025-12-26T12:20:30.439735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 11) 5-fold OOF XGBoost (AUC, sparse-friendly) — xgboost==3.1.2 compatible\n# ----------------------------\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\nX_full_raw = train_df.drop(columns=[TARGET_COL, ID_COL])\ny_full = train_df[TARGET_COL].values.astype(np.int32)\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\noof_pred = np.zeros(len(y_full), dtype=np.float32)\nfold_scores, best_iters = [], []\n\n# Base params (per-fold scale_pos_weight will be computed inside the loop)\nxgb_params = dict(\n    n_estimators=5000,\n    learning_rate=0.03,\n    max_depth=4,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_lambda=1.0,\n    objective=\"binary:logistic\",\n    eval_metric=\"auc\",\n    tree_method=\"hist\",        # GPU可用时：device=\"cuda\"\n    device=\"cuda\", \n    n_jobs=-1,\n    random_state=SEED,\n    early_stopping_rounds=200, # ✅ xgboost>=2.0/3.x：放在构造函数里，不要放 fit()\n)\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X_full_raw, y_full), start=1):\n    X_tr_raw, X_val_raw = X_full_raw.iloc[tr_idx], X_full_raw.iloc[val_idx]\n    y_tr, y_val_fold = y_full[tr_idx], y_full[val_idx]\n\n    # 1) Fold-specific preprocessing (fit on train fold only)\n    pre_fold = clone(preprocess)\n    X_tr_proc = pre_fold.fit_transform(X_tr_raw)\n    X_val_proc = pre_fold.transform(X_val_raw)\n\n    # Keep float32 (works for numpy arrays and scipy sparse matrices)\n    X_tr_proc = X_tr_proc.astype(np.float32)\n    X_val_proc = X_val_proc.astype(np.float32)\n\n    # 2) Fold-specific class weight (more correct than using global)\n    pos = int((y_tr == 1).sum())\n    neg = int((y_tr == 0).sum())\n    scale_pos = neg / max(pos, 1)\n\n    # 3) Train model\n    model = xgb.XGBClassifier(**xgb_params, scale_pos_weight=scale_pos)\n\n    model.fit(\n        X_tr_proc, y_tr,\n        eval_set=[(X_val_proc, y_val_fold)],\n        verbose=50,\n    )\n\n    # 4) Predict using best_iteration (important even if model stored all trees)\n    best_iter = getattr(model, \"best_iteration\", None)\n    if best_iter is not None:\n        fold_pred = model.predict_proba(X_val_proc, iteration_range=(0, best_iter + 1))[:, 1]\n    else:\n        fold_pred = model.predict_proba(X_val_proc)[:, 1]\n\n    oof_pred[val_idx] = fold_pred\n    fold_auc = roc_auc_score(y_val_fold, fold_pred)\n\n    best_iters.append(int(best_iter) if best_iter is not None else 0)\n    fold_scores.append(float(fold_auc))\n    print(f\"Fold {fold}: AUC={fold_auc:.5f}, best_iter={best_iters[-1]}\")\n\noof_auc = roc_auc_score(y_full, oof_pred)\nvalid_best_iters = [b for b in best_iters if b > 0]\navg_best_iter = int(np.mean(valid_best_iters)) if valid_best_iters else 500\n\nprint(f\"OOF AUC: {oof_auc:.5f}\")\nprint(f\"Fold AUCs: {[round(s, 5) for s in fold_scores]}\")\nprint(f\"Avg best_iter (rounded): {avg_best_iter}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:22:25.953921Z","iopub.execute_input":"2025-12-26T12:22:25.954522Z","iopub.status.idle":"2025-12-26T12:24:08.168426Z","shell.execute_reply.started":"2025-12-26T12:22:25.954492Z","shell.execute_reply":"2025-12-26T12:24:08.167584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 12) Final train on full data + generate submission (XGBoost 3.1.2 compatible)\n# ----------------------------\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.base import clone\n\n# Ensure raw matrices exist even if previous cell wasn't run\nX_full_raw = train_df.drop(columns=[TARGET_COL, ID_COL])\ny_full = train_df[TARGET_COL].values.astype(np.int32)\nX_test_raw = test_df.drop(columns=[ID_COL])\n\nif 'test_ids' not in locals():\n    test_ids = test_df[ID_COL].values\n\n# Recompute class weight and xgb_params if missing\nscale_pos_final = (y_full == 0).sum() / (y_full == 1).sum()\n\nif 'xgb_params' not in locals():\n    xgb_params = dict(\n        n_estimators=5000,\n        learning_rate=0.03,\n        max_depth=4,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=1.0,\n        objective=\"binary:logistic\",\n        eval_metric=\"auc\",\n        tree_method=\"hist\",\n        scale_pos_weight=scale_pos_final,\n        n_jobs=-1,\n        random_state=SEED,\n    )\nelse:\n    xgb_params = dict(xgb_params)\n    xgb_params['scale_pos_weight'] = scale_pos_final\n\n# ---- NEW: final params (remove early stopping related settings) ----\nxgb_params_final = dict(xgb_params)\nxgb_params_final.pop(\"early_stopping_rounds\", None)  # ✅ must remove for full fit without eval_set\nxgb_params_final.pop(\"callbacks\", None)              # ✅ just in case\n\n# ---- NEW: try to enable GPU safely (xgboost 3.x) ----\nuse_cuda = bool(xgb.build_info().get(\"USE_CUDA\", False))\nif use_cuda:\n    xgb_params_final[\"tree_method\"] = \"hist\"\n    xgb_params_final[\"device\"] = \"cuda\"  # or \"cuda:0\"\n    print(\"[XGBoost] GPU enabled: device=cuda\")\nelse:\n    # CPU fallback\n    xgb_params_final.pop(\"device\", None)\n    xgb_params_final[\"tree_method\"] = \"hist\"\n    print(\"[XGBoost] GPU not available, using CPU\")\n\n# Fit preprocessing on full train, transform test\npreprocess_final = clone(preprocess)\nX_full_proc = preprocess_final.fit_transform(X_full_raw).astype(np.float32)\nX_test_proc_final = preprocess_final.transform(X_test_raw).astype(np.float32)\n\n# Use avg_best_iter from OOF if available; otherwise fallback\nfinal_n_estimators = int(max(avg_best_iter, 50)) if 'avg_best_iter' in locals() else 500\n\n# Train final model on full data\n#model_final = xgb.XGBClassifier(**xgb_params_final, n_estimators=final_n_estimators)\n#model_final.fit(X_full_proc, y_full, verbose=200)\n\nxgb_params_final['n_estimators'] = final_n_estimators\n\nmodel_final = xgb.XGBClassifier(**xgb_params_final) # 这样就不会报错了\nmodel_final.fit(X_full_proc, y_full, verbose=100)\n\n# Predict + submission\ntest_pred = model_final.predict_proba(X_test_proc_final)[:, 1]\n\nsubmission = pd.DataFrame({ID_COL: test_ids, TARGET_COL: test_pred})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(f\"Saved submission_xgb.csv with shape {submission.shape} and n_estimators={final_n_estimators}\")\ndisplay(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:40:53.162593Z","iopub.execute_input":"2025-12-26T12:40:53.163184Z","iopub.status.idle":"2025-12-26T12:41:13.182858Z","shell.execute_reply.started":"2025-12-26T12:40:53.163155Z","shell.execute_reply":"2025-12-26T12:41:13.182103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assert submission.shape[0] == 300000\nassert list(submission.columns) == [\"id\", \"diagnosed_diabetes\"]\nassert submission[\"diagnosed_diabetes\"].between(0, 1).all()\nassert submission[\"id\"].is_unique\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T12:33:59.087448Z","iopub.execute_input":"2025-12-26T12:33:59.087748Z","iopub.status.idle":"2025-12-26T12:33:59.099740Z","shell.execute_reply.started":"2025-12-26T12:33:59.087722Z","shell.execute_reply":"2025-12-26T12:33:59.099107Z"}},"outputs":[],"execution_count":null}]}